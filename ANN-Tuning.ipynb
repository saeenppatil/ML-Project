{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f380d29-082f-4712-bb7c-beb42355b6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with Layers: (64, 32) and Batch Size: 32\n",
      "Epoch 1: Train Accuracy = 0.5890\n",
      "Epoch 2: Train Accuracy = 0.8637\n",
      "Epoch 3: Train Accuracy = 0.9055\n",
      "Epoch 4: Train Accuracy = 0.9209\n",
      "Epoch 5: Train Accuracy = 0.9363\n",
      "Epoch 6: Train Accuracy = 0.9560\n",
      "Epoch 7: Train Accuracy = 0.9626\n",
      "Epoch 8: Train Accuracy = 0.9516\n",
      "Epoch 9: Train Accuracy = 0.9604\n",
      "Epoch 10: Train Accuracy = 0.9604\n",
      "Epoch 11: Train Accuracy = 0.9538\n",
      "Epoch 12: Train Accuracy = 0.9604\n",
      "Epoch 13: Train Accuracy = 0.9516\n",
      "Epoch 14: Train Accuracy = 0.9582\n",
      "Epoch 15: Train Accuracy = 0.9648\n",
      "Epoch 16: Train Accuracy = 0.9626\n",
      "Epoch 17: Train Accuracy = 0.9604\n",
      "Epoch 18: Train Accuracy = 0.9604\n",
      "Epoch 19: Train Accuracy = 0.9626\n",
      "Epoch 20: Train Accuracy = 0.9560\n",
      "Epoch 21: Train Accuracy = 0.9670\n",
      "Epoch 22: Train Accuracy = 0.9670\n",
      "Epoch 23: Train Accuracy = 0.9670\n",
      "Epoch 24: Train Accuracy = 0.9670\n",
      "Epoch 25: Train Accuracy = 0.9648\n",
      "Epoch 26: Train Accuracy = 0.9714\n",
      "Epoch 27: Train Accuracy = 0.9692\n",
      "Epoch 28: Train Accuracy = 0.9758\n",
      "Epoch 29: Train Accuracy = 0.9670\n",
      "Epoch 30: Train Accuracy = 0.9692\n",
      "Epoch 31: Train Accuracy = 0.9846\n",
      "Epoch 32: Train Accuracy = 0.9780\n",
      "Epoch 33: Train Accuracy = 0.9736\n",
      "Epoch 34: Train Accuracy = 0.9736\n",
      "Epoch 35: Train Accuracy = 0.9736\n",
      "Epoch 36: Train Accuracy = 0.9758\n",
      "Epoch 37: Train Accuracy = 0.9714\n",
      "Epoch 38: Train Accuracy = 0.9626\n",
      "Epoch 39: Train Accuracy = 0.9714\n",
      "Epoch 40: Train Accuracy = 0.9736\n",
      "Epoch 41: Train Accuracy = 0.9736\n",
      "Epoch 42: Train Accuracy = 0.9648\n",
      "Epoch 43: Train Accuracy = 0.9648\n",
      "Epoch 44: Train Accuracy = 0.9648\n",
      "Epoch 45: Train Accuracy = 0.9736\n",
      "Epoch 46: Train Accuracy = 0.9736\n",
      "Epoch 47: Train Accuracy = 0.9648\n",
      "Epoch 48: Train Accuracy = 0.9714\n",
      "Epoch 49: Train Accuracy = 0.9648\n",
      "Epoch 50: Train Accuracy = 0.9670\n",
      "Test Accuracy: 0.9649\n",
      "\n",
      "Training with Layers: (128, 64) and Batch Size: 32\n",
      "Epoch 1: Train Accuracy = 0.7099\n",
      "Epoch 2: Train Accuracy = 0.8923\n",
      "Epoch 3: Train Accuracy = 0.9209\n",
      "Epoch 4: Train Accuracy = 0.9538\n",
      "Epoch 5: Train Accuracy = 0.9495\n",
      "Epoch 6: Train Accuracy = 0.9582\n",
      "Epoch 7: Train Accuracy = 0.9560\n",
      "Epoch 8: Train Accuracy = 0.9692\n",
      "Epoch 9: Train Accuracy = 0.9604\n",
      "Epoch 10: Train Accuracy = 0.9692\n",
      "Epoch 11: Train Accuracy = 0.9714\n",
      "Epoch 12: Train Accuracy = 0.9714\n",
      "Epoch 13: Train Accuracy = 0.9692\n",
      "Epoch 14: Train Accuracy = 0.9604\n",
      "Epoch 15: Train Accuracy = 0.9670\n",
      "Epoch 16: Train Accuracy = 0.9714\n",
      "Epoch 17: Train Accuracy = 0.9692\n",
      "Epoch 18: Train Accuracy = 0.9692\n",
      "Epoch 19: Train Accuracy = 0.9714\n",
      "Epoch 20: Train Accuracy = 0.9736\n",
      "Epoch 21: Train Accuracy = 0.9692\n",
      "Epoch 22: Train Accuracy = 0.9626\n",
      "Epoch 23: Train Accuracy = 0.9648\n",
      "Epoch 24: Train Accuracy = 0.9692\n",
      "Epoch 25: Train Accuracy = 0.9670\n",
      "Epoch 26: Train Accuracy = 0.9692\n",
      "Epoch 27: Train Accuracy = 0.9714\n",
      "Epoch 28: Train Accuracy = 0.9714\n",
      "Epoch 29: Train Accuracy = 0.9670\n",
      "Epoch 30: Train Accuracy = 0.9714\n",
      "Epoch 31: Train Accuracy = 0.9648\n",
      "Epoch 32: Train Accuracy = 0.9714\n",
      "Epoch 33: Train Accuracy = 0.9516\n",
      "Epoch 34: Train Accuracy = 0.9604\n",
      "Epoch 35: Train Accuracy = 0.9626\n",
      "Epoch 36: Train Accuracy = 0.9516\n",
      "Epoch 37: Train Accuracy = 0.9648\n",
      "Epoch 38: Train Accuracy = 0.9604\n",
      "Epoch 39: Train Accuracy = 0.9604\n",
      "Epoch 40: Train Accuracy = 0.9604\n",
      "Epoch 41: Train Accuracy = 0.9604\n",
      "Epoch 42: Train Accuracy = 0.9670\n",
      "Epoch 43: Train Accuracy = 0.9582\n",
      "Epoch 44: Train Accuracy = 0.9648\n",
      "Epoch 45: Train Accuracy = 0.9604\n",
      "Epoch 46: Train Accuracy = 0.9626\n",
      "Epoch 47: Train Accuracy = 0.9604\n",
      "Epoch 48: Train Accuracy = 0.9560\n",
      "Epoch 49: Train Accuracy = 0.9473\n",
      "Epoch 50: Train Accuracy = 0.9670\n",
      "Test Accuracy: 0.9649\n",
      "\n",
      "Final Results:\n",
      "      layers  batch_size  train_accuracy  test_accuracy\n",
      "0   (64, 32)          32        0.967033       0.964912\n",
      "1  (128, 64)          32        0.967033       0.964912\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "\n",
    "data.drop(['id', 'Unnamed: 32'], axis=1, inplace=True)\n",
    "\n",
    "data['diagnosis'] = data['diagnosis'].map({'M': 1, 'B': 0})\n",
    "\n",
    "features = [\n",
    "    'radius_mean', 'texture_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
    "    'symmetry_mean', 'radius_se', 'concave points_se', 'smoothness_worst',\n",
    "    'compactness_worst', 'concavity_worst', 'symmetry_worst', 'fractal_dimension_worst'\n",
    "]\n",
    "\n",
    "X = data[features]\n",
    "y = data['diagnosis']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "layer_configs = [\n",
    "    (64, 32),  \n",
    "    (128, 64)  \n",
    "]\n",
    "\n",
    "batch_sizes = [32]  \n",
    "epochs = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "results = []  #\n",
    "\n",
    "class AccuracyLogger(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        train_acc = logs.get('accuracy')\n",
    "        print(f\"Epoch {epoch + 1}: Train Accuracy = {train_acc:.4f}\")\n",
    "\n",
    "for layers in layer_configs:\n",
    "    for batch_size in batch_sizes:\n",
    "        print(f\"\\nTraining with Layers: {layers} and Batch Size: {batch_size}\")\n",
    "        \n",
    "        model = Sequential([\n",
    "            Input(shape=(X_train.shape[1],)),  \n",
    "            Dense(layers[0], activation='relu'),  \n",
    "            Dropout(0.3),\n",
    "            Dense(layers[1], activation='relu'), \n",
    "            Dropout(0.3),\n",
    "            Dense(1, activation='sigmoid')  \n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        history = model.fit(X_train, y_train,\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            callbacks=[AccuracyLogger()],\n",
    "                            verbose=0)\n",
    "\n",
    "        test_accuracy = model.evaluate(X_test, y_test, verbose=0)[1]\n",
    "        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "        results.append({\n",
    "            'layers': layers,\n",
    "            'batch_size': batch_size,\n",
    "            'train_accuracy': history.history['accuracy'][-1],\n",
    "            'test_accuracy': test_accuracy\n",
    "        })\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nFinal Results:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d1b36cd-6cbc-48b6-a4a8-597343c87a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameter combinations to test: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning Progress: 100%|█████████████| 1/1 [00:44<00:00, 44.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Hyperparameters and Score:\n",
      "learning_rate     0.10000\n",
      "dropout_rate      0.20000\n",
      "batch_size        8.00000\n",
      "epochs           30.00000\n",
      "best_score        0.96041\n",
      "Name: 0, dtype: float64\n",
      "Test Accuracy with Best Hyperparameters: 0.9386\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "data.drop(['id', 'Unnamed: 32'], axis=1, inplace=True) \n",
    "data['diagnosis'] = data['diagnosis'].map({'M': 1, 'B': 0}) \n",
    "\n",
    "features = [\n",
    "    'radius_mean', 'texture_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
    "    'symmetry_mean', 'radius_se', 'concave points_se', 'smoothness_worst',\n",
    "    'compactness_worst', 'concavity_worst', 'symmetry_worst', 'fractal_dimension_worst'\n",
    "]\n",
    "\n",
    "X = data[features]\n",
    "y = data['diagnosis']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "def create_model(learning_rate, dropout_rate):\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train.shape[1],)), \n",
    "        Dense(64, activation='relu'),    \n",
    "        Dropout(dropout_rate),            \n",
    "        Dense(32, activation='relu'),    \n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='sigmoid')   \n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = KerasClassifier(\n",
    "    model=create_model,\n",
    "    model__learning_rate=0.001, \n",
    "    model__dropout_rate=0.3,   \n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'model__learning_rate': [0.1],  \n",
    "    'model__dropout_rate': [0.2],        \n",
    "    'batch_size': [8],                       \n",
    "    'epochs': [30]                                \n",
    "}\n",
    "\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "param_combinations = list(product(\n",
    "    param_grid['model__learning_rate'],\n",
    "    param_grid['model__dropout_rate'],\n",
    "    param_grid['batch_size'],\n",
    "    param_grid['epochs']\n",
    "))\n",
    "\n",
    "print(f\"Total parameter combinations to test: {len(param_combinations)}\")\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, (lr, dr, bs, ep) in enumerate(tqdm(param_combinations, desc=\"Hyperparameter Tuning Progress\"), start=1):\n",
    "    temp_grid = {\n",
    "        'model__learning_rate': [lr],\n",
    "        'model__dropout_rate': [dr],\n",
    "        'batch_size': [bs],\n",
    "        'epochs': [ep]\n",
    "    }\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=temp_grid,\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        scoring='accuracy',\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "    results.append({\n",
    "        'learning_rate': lr,\n",
    "        'dropout_rate': dr,\n",
    "        'batch_size': bs,\n",
    "        'epochs': ep,\n",
    "        'best_score': grid_result.best_score_\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "best_row = results_df.loc[results_df['best_score'].idxmax()]\n",
    "\n",
    "print(\"\\nBest Hyperparameters and Score:\")\n",
    "print(best_row)\n",
    "\n",
    "best_model = KerasClassifier(\n",
    "    model=create_model,\n",
    "    model__learning_rate=best_row['learning_rate'],\n",
    "    model__dropout_rate=best_row['dropout_rate'],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "best_model.fit(X_train, y_train, batch_size=int(best_row['batch_size']), epochs=int(best_row['epochs']), verbose=0)\n",
    "test_accuracy = best_model.score(X_test, y_test)\n",
    "print(f\"Test Accuracy with Best Hyperparameters: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "421d1da3-12ad-4ae8-93f5-1b06947c968c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameter combinations to test: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning Progress: 100%|█████████████| 5/5 [04:38<00:00, 55.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Hyperparameters and Score:\n",
      "learning_rate     0.001000\n",
      "dropout_rate      0.200000\n",
      "batch_size        8.000000\n",
      "epochs           30.000000\n",
      "best_score        0.975834\n",
      "Name: 2, dtype: float64\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x564acc310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 12 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x564acc310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Test Accuracy with Best Hyperparameters: 0.9561\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "data.drop(['id', 'Unnamed: 32'], axis=1, inplace=True)  \n",
    "data['diagnosis'] = data['diagnosis'].map({'M': 1, 'B': 0}) \n",
    "\n",
    "features = [\n",
    "    'radius_mean', 'texture_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
    "    'symmetry_mean', 'radius_se', 'concave points_se', 'smoothness_worst',\n",
    "    'compactness_worst', 'concavity_worst', 'symmetry_worst', 'fractal_dimension_worst'\n",
    "]\n",
    "\n",
    "X = data[features]\n",
    "y = data['diagnosis']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "def create_model(learning_rate, dropout_rate):\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train.shape[1],)), \n",
    "        Dense(64, activation='relu'),   \n",
    "        Dropout(dropout_rate),           \n",
    "        Dense(32, activation='relu'),    \n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='sigmoid')   \n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(\n",
    "    model=create_model,\n",
    "    model__learning_rate=0.001, \n",
    "    model__dropout_rate=0.3,   \n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    'model__learning_rate': [1.0, 0.1, 0.001, 0.01, 0.0001],  \n",
    "    'model__dropout_rate': [0.2],        \n",
    "    'batch_size': [8],                       \n",
    "    'epochs': [30]                                \n",
    "}\n",
    "\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "param_combinations = list(product(\n",
    "    param_grid['model__learning_rate'],\n",
    "    param_grid['model__dropout_rate'],\n",
    "    param_grid['batch_size'],\n",
    "    param_grid['epochs']\n",
    "))\n",
    "\n",
    "print(f\"Total parameter combinations to test: {len(param_combinations)}\")\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, (lr, dr, bs, ep) in enumerate(tqdm(param_combinations, desc=\"Hyperparameter Tuning Progress\"), start=1):\n",
    "    temp_grid = {\n",
    "        'model__learning_rate': [lr],\n",
    "        'model__dropout_rate': [dr],\n",
    "        'batch_size': [bs],\n",
    "        'epochs': [ep]\n",
    "    }\n",
    "\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=temp_grid,\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        scoring='accuracy',\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "    results.append({\n",
    "        'learning_rate': lr,\n",
    "        'dropout_rate': dr,\n",
    "        'batch_size': bs,\n",
    "        'epochs': ep,\n",
    "        'best_score': grid_result.best_score_\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "best_row = results_df.loc[results_df['best_score'].idxmax()]\n",
    "\n",
    "print(\"\\nBest Hyperparameters and Score:\")\n",
    "print(best_row)\n",
    "\n",
    "\n",
    "best_model = KerasClassifier(\n",
    "    model=create_model,\n",
    "    model__learning_rate=best_row['learning_rate'],\n",
    "    model__dropout_rate=best_row['dropout_rate'],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "best_model.fit(X_train, y_train, batch_size=int(best_row['batch_size']), epochs=int(best_row['epochs']), verbose=0)\n",
    "test_accuracy = best_model.score(X_test, y_test)\n",
    "print(f\"Test Accuracy with Best Hyperparameters: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "910130a1-5f50-412b-b0bd-97659de6370f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameter combinations to test: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning Progress: 100%|█████████████| 3/3 [02:16<00:00, 45.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Hyperparameters and Score:\n",
      "learning_rate     0.100000\n",
      "dropout_rate      0.300000\n",
      "batch_size        8.000000\n",
      "epochs           30.000000\n",
      "best_score        0.964811\n",
      "Name: 1, dtype: float64\n",
      "Test Accuracy with Best Hyperparameters: 0.9649\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "data.drop(['id', 'Unnamed: 32'], axis=1, inplace=True)  \n",
    "data['diagnosis'] = data['diagnosis'].map({'M': 1, 'B': 0}) \n",
    "\n",
    "features = [\n",
    "    'radius_mean', 'texture_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
    "    'symmetry_mean', 'radius_se', 'concave points_se', 'smoothness_worst',\n",
    "    'compactness_worst', 'concavity_worst', 'symmetry_worst', 'fractal_dimension_worst'\n",
    "]\n",
    "\n",
    "X = data[features]\n",
    "y = data['diagnosis']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "def create_model(learning_rate, dropout_rate):\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train.shape[1],)), \n",
    "        Dense(64, activation='relu'),    \n",
    "        Dropout(dropout_rate),           \n",
    "        Dense(32, activation='relu'),     \n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='sigmoid')    \n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(\n",
    "    model=create_model,\n",
    "    model__learning_rate=0.001,  \n",
    "    model__dropout_rate=0.3,    \n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    'model__learning_rate': [0.1],  \n",
    "    'model__dropout_rate': [0.2, 0.3, 0.4],        \n",
    "    'batch_size': [8],                       \n",
    "    'epochs': [30]                                \n",
    "}\n",
    "\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "param_combinations = list(product(\n",
    "    param_grid['model__learning_rate'],\n",
    "    param_grid['model__dropout_rate'],\n",
    "    param_grid['batch_size'],\n",
    "    param_grid['epochs']\n",
    "))\n",
    "\n",
    "print(f\"Total parameter combinations to test: {len(param_combinations)}\")\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, (lr, dr, bs, ep) in enumerate(tqdm(param_combinations, desc=\"Hyperparameter Tuning Progress\"), start=1):\n",
    "    temp_grid = {\n",
    "        'model__learning_rate': [lr],\n",
    "        'model__dropout_rate': [dr],\n",
    "        'batch_size': [bs],\n",
    "        'epochs': [ep]\n",
    "    }\n",
    "\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=temp_grid,\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        scoring='accuracy',\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "    results.append({\n",
    "        'learning_rate': lr,\n",
    "        'dropout_rate': dr,\n",
    "        'batch_size': bs,\n",
    "        'epochs': ep,\n",
    "        'best_score': grid_result.best_score_\n",
    "    })\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "best_row = results_df.loc[results_df['best_score'].idxmax()]\n",
    "\n",
    "print(\"\\nBest Hyperparameters and Score:\")\n",
    "print(best_row)\n",
    "\n",
    "\n",
    "best_model = KerasClassifier(\n",
    "    model=create_model,\n",
    "    model__learning_rate=best_row['learning_rate'],\n",
    "    model__dropout_rate=best_row['dropout_rate'],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "\n",
    "best_model.fit(X_train, y_train, batch_size=int(best_row['batch_size']), epochs=int(best_row['epochs']), verbose=0)\n",
    "test_accuracy = best_model.score(X_test, y_test)\n",
    "print(f\"Test Accuracy with Best Hyperparameters: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5c34d59-fd8e-467e-bcc8-6f75e8d09a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameter combinations to test: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning Progress: 100%|█████████████| 3/3 [01:30<00:00, 30.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Hyperparameters and Score:\n",
      "learning_rate     0.10000\n",
      "dropout_rate      0.20000\n",
      "batch_size        8.00000\n",
      "epochs           30.00000\n",
      "best_score        0.96041\n",
      "Name: 0, dtype: float64\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x342484790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 12 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x342484790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Test Accuracy with Best Hyperparameters: 0.9474\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "data.drop(['id', 'Unnamed: 32'], axis=1, inplace=True) \n",
    "data['diagnosis'] = data['diagnosis'].map({'M': 1, 'B': 0}) \n",
    "\n",
    "features = [\n",
    "    'radius_mean', 'texture_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
    "    'symmetry_mean', 'radius_se', 'concave points_se', 'smoothness_worst',\n",
    "    'compactness_worst', 'concavity_worst', 'symmetry_worst', 'fractal_dimension_worst'\n",
    "]\n",
    "\n",
    "X = data[features]\n",
    "y = data['diagnosis']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "def create_model(learning_rate, dropout_rate):\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train.shape[1],)), \n",
    "        Dense(64, activation='relu'),    \n",
    "        Dropout(dropout_rate),          \n",
    "        Dense(32, activation='relu'),    \n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='sigmoid')   \n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(\n",
    "    model=create_model,\n",
    "    model__learning_rate=0.001, \n",
    "    model__dropout_rate=0.3,    \n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'model__learning_rate': [0.1],  \n",
    "    'model__dropout_rate': [0.2],        \n",
    "    'batch_size': [8, 16, 32],                       \n",
    "    'epochs': [30]                                \n",
    "}\n",
    "\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "param_combinations = list(product(\n",
    "    param_grid['model__learning_rate'],\n",
    "    param_grid['model__dropout_rate'],\n",
    "    param_grid['batch_size'],\n",
    "    param_grid['epochs']\n",
    "))\n",
    "\n",
    "print(f\"Total parameter combinations to test: {len(param_combinations)}\")\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, (lr, dr, bs, ep) in enumerate(tqdm(param_combinations, desc=\"Hyperparameter Tuning Progress\"), start=1):\n",
    "    temp_grid = {\n",
    "        'model__learning_rate': [lr],\n",
    "        'model__dropout_rate': [dr],\n",
    "        'batch_size': [bs],\n",
    "        'epochs': [ep]\n",
    "    }\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=temp_grid,\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        scoring='accuracy',\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "    results.append({\n",
    "        'learning_rate': lr,\n",
    "        'dropout_rate': dr,\n",
    "        'batch_size': bs,\n",
    "        'epochs': ep,\n",
    "        'best_score': grid_result.best_score_\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "best_row = results_df.loc[results_df['best_score'].idxmax()]\n",
    "\n",
    "print(\"\\nBest Hyperparameters and Score:\")\n",
    "print(best_row)\n",
    "\n",
    "\n",
    "best_model = KerasClassifier(\n",
    "    model=create_model,\n",
    "    model__learning_rate=best_row['learning_rate'],\n",
    "    model__dropout_rate=best_row['dropout_rate'],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "\n",
    "best_model.fit(X_train, y_train, batch_size=int(best_row['batch_size']), epochs=int(best_row['epochs']), verbose=0)\n",
    "test_accuracy = best_model.score(X_test, y_test)\n",
    "print(f\"Test Accuracy with Best Hyperparameters: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6d3c451-fdcd-4bed-9046-38a00aa15aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameter combinations to test: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning Progress:   0%|                     | 0/6 [00:00<?, ?it/s]WARNING:tensorflow:5 out of the last 16 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x2a12acd30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Hyperparameter Tuning Progress: 100%|████████████| 6/6 [10:56<00:00, 109.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Hyperparameters and Score:\n",
      "learning_rate     0.100000\n",
      "dropout_rate      0.200000\n",
      "batch_size        8.000000\n",
      "epochs           30.000000\n",
      "best_score        0.969226\n",
      "Name: 1, dtype: float64\n",
      "Test Accuracy with Best Hyperparameters: 0.9298\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "data.drop(['id', 'Unnamed: 32'], axis=1, inplace=True) \n",
    "data['diagnosis'] = data['diagnosis'].map({'M': 1, 'B': 0}) \n",
    "\n",
    "features = [\n",
    "    'radius_mean', 'texture_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
    "    'symmetry_mean', 'radius_se', 'concave points_se', 'smoothness_worst',\n",
    "    'compactness_worst', 'concavity_worst', 'symmetry_worst', 'fractal_dimension_worst'\n",
    "]\n",
    "\n",
    "X = data[features]\n",
    "y = data['diagnosis']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "def create_model(learning_rate, dropout_rate):\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train.shape[1],)),\n",
    "        Dense(64, activation='relu'),    \n",
    "        Dropout(dropout_rate),           \n",
    "        Dense(32, activation='relu'),     \n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='sigmoid')   \n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(\n",
    "    model=create_model,\n",
    "    model__learning_rate=0.001,  \n",
    "    model__dropout_rate=0.3,   \n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'model__learning_rate': [0.1],  \n",
    "    'model__dropout_rate': [0.2],        \n",
    "    'batch_size': [8],                       \n",
    "    'epochs': [20, 30, 50, 70, 100, 200]                                \n",
    "}\n",
    "\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "param_combinations = list(product(\n",
    "    param_grid['model__learning_rate'],\n",
    "    param_grid['model__dropout_rate'],\n",
    "    param_grid['batch_size'],\n",
    "    param_grid['epochs']\n",
    "))\n",
    "\n",
    "print(f\"Total parameter combinations to test: {len(param_combinations)}\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, (lr, dr, bs, ep) in enumerate(tqdm(param_combinations, desc=\"Hyperparameter Tuning Progress\"), start=1):\n",
    "    temp_grid = {\n",
    "        'model__learning_rate': [lr],\n",
    "        'model__dropout_rate': [dr],\n",
    "        'batch_size': [bs],\n",
    "        'epochs': [ep]\n",
    "    }\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=temp_grid,\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        scoring='accuracy',\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "    results.append({\n",
    "        'learning_rate': lr,\n",
    "        'dropout_rate': dr,\n",
    "        'batch_size': bs,\n",
    "        'epochs': ep,\n",
    "        'best_score': grid_result.best_score_\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "best_row = results_df.loc[results_df['best_score'].idxmax()]\n",
    "\n",
    "print(\"\\nBest Hyperparameters and Score:\")\n",
    "print(best_row)\n",
    "\n",
    "best_model = KerasClassifier(\n",
    "    model=create_model,\n",
    "    model__learning_rate=best_row['learning_rate'],\n",
    "    model__dropout_rate=best_row['dropout_rate'],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "best_model.fit(X_train, y_train, batch_size=int(best_row['batch_size']), epochs=int(best_row['epochs']), verbose=0)\n",
    "test_accuracy = best_model.score(X_test, y_test)\n",
    "print(f\"Test Accuracy with Best Hyperparameters: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03090b3-b4c0-4cd7-a89e-3a9735dbe1c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b688ab1-1621-421b-9395-8f63513f279b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
